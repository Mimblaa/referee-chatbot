{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "def clean_for_uri(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[ąćęłńóśźż]\", lambda m: {\n",
    "        'ą': 'a', 'ć': 'c', 'ę': 'e', 'ł': 'l',\n",
    "        'ń': 'n', 'ó': 'o', 'ś': 's', 'ź': 'z', 'ż': 'z'\n",
    "    }[m.group()], text)\n",
    "    text = text.replace(\" \", \"_\")\n",
    "    text = re.sub(r\"[^\\w\\-]\", \"\", text)\n",
    "    return urllib.parse.quote(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('pl')\n",
    "nlp_stanza = stanza.Pipeline('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {\n",
    "    \"LINIA\": [\n",
    "        \"linia\", \"linia boczna\", \"linia bramkowa\", \"linia środkowa\", \n",
    "        \"punkt\", \"punkt karny\", \"punkt środkowy\", \n",
    "        \"linia pola karnego\", \"linia pola bramkowego\", \"linia pola rożnego\", \n",
    "        \"łuk pola karnego\", \"łuk pola rożnego\", \"linia spalonego\"\n",
    "    ],\n",
    "    \"POZYCJA\": [\n",
    "        \"zawodnik\", \"bramkarz\", \"obrońca\", \"środkowy obrońca\", \"boczny obrońca\",\n",
    "        \"pomocnik\", \"pomocnik defensywny\", \"pomocnik ofensywny\", \"skrzydłowy\",\n",
    "        \"napastnik\", \"rezerwowy\", \"zawodnik wymieniony\", \"kapitan drużyny\",\n",
    "        \"trener\", \"asystent trenera\", \"sędzia\", \"sędzia główny\", \n",
    "        \"sędzia asystent\", \"sędzia techniczny\", \"sędzia VAR\", \n",
    "        \"operator VAR\", \"lekarz drużyny\", \"fizjoterapeuta\", \"kierownik drużyny\"\n",
    "    ],\n",
    "    \"SPRZĘT\": [\n",
    "        \"sprzęt\", \"piłka\", \"piłka meczowa\", \"buty\", \"korki\", \"strój\", \n",
    "        \"getry\", \"ochraniacze\", \"ochraniacze goleni\", \"rękawice bramkarskie\",\n",
    "        \"chorągiewka\", \"chorągiewka boczna\", \"bramka\", \"siatka\", \n",
    "        \"poprzeczka\", \"słupek\", \"tablica zmian\", \"gwizdek\", \"zegarek sędziego\", \n",
    "        \"kamera VAR\", \"monitor VAR\", \"system GLT\", \"system VAR\"\n",
    "    ],\n",
    "    \"CZYNNOŚĆ\": [\n",
    "        \"czynność\", \"rozpoczęcie gry\", \"wznowienie gry\", \"rzut karny\", \n",
    "        \"rzut wolny\", \"rzut wolny pośredni\", \"rzut wolny bezpośredni\", \n",
    "        \"rzut z autu\", \"rzut rożny\", \"rzut od bramki\", \"dogrywka\", \n",
    "        \"rzuty karne\", \"wrzut\", \"drybling\", \"pressing\", \"odbiór piłki\", \n",
    "        \"strzał\", \"strzał na bramkę\", \"podanie\", \"asysta\", \"zagranie ręką\",\n",
    "        \"przewinienie\", \"przewinienie taktyczne\", \"symulacja\", \"opóźnianie gry\",\n",
    "        \"brutalność\", \"gra niebezpieczna\", \"korzyść sędziowska\", \"wideoweryfikacja\"\n",
    "    ],\n",
    "    \"CZAS\": [\n",
    "        \"czas\", \"czas gry\", \"pierwsza połowa\", \"druga połowa\", \n",
    "        \"doliczony czas\", \"doliczony czas gry\", \"dogrywka\", \"rzuty karne\", \n",
    "        \"seria rzutów karnych\", \"przerwa\", \"przerwa w grze\", \n",
    "        \"przerwa na chłodzenie\", \"czas zawieszenia\", \"czas przewinienia\",\n",
    "        \"rozmowa pomeczowa\", \"czas na zmianę\", \"raport sędziowski\",\n",
    "    ],\n",
    "    \"ORGANIZACJA\": [\n",
    "        \"organizacja\", \"IFAB\", \"FIFA\", \"UEFA\", \"PZPN\", \"konfederacja\", \n",
    "        \"federacja krajowa\", \"komisja sędziowska\", \"organizator rozgrywek\"\n",
    "    ],\n",
    "    \"SANKCJA\": [\n",
    "        \"sankcja\", \"żółta kartka\", \"czerwona kartka\", \"napomnienie\", \n",
    "        \"wykluczenie\", \"kara wychowawcza\", \"zawieszenie czasowe\", \n",
    "        \"zawieszenie meczowe\", \"kara techniczna\", \"upomnienie słowne\"\n",
    "    ],\n",
    "    \"OSOBA_FUNCKYJNA\": [\n",
    "        \"osoba funkcyjna\", \"trener\", \"asystent trenera\", \"fizjoterapeuta\", \n",
    "        \"lekarz drużyny\", \"analityk wideo\", \"kierownik drużyny\", \n",
    "        \"sędzia techniczny\", \"delegat meczu\", \"operator VAR\",\n",
    "        \"sędzia VAR\", \"sędzia asystent VAR\", \"obserwator\",\n",
    "        \"sędzia główna\", \"sędzia boczny\", \"sędzia asystent\"\n",
    "        \"sędzia liniowy\", \"sędzia techniczny\", \"sędzia IV\",\n",
    "    ],\n",
    "\n",
    "    \"STAN_GRY\": [\n",
    "        \"stan gry\", \"spalony\", \"przewinienie\", \"faul\", \"symulacja\", \n",
    "        \"opóźnianie gry\", \"brutalność\", \"gra niebezpieczna\", \n",
    "        \"korzyść sędziowska\", \"nieuznana bramka\", \"kontuzja zawodnika\",\n",
    "    ],\n",
    "    \"DECYZJA_SEDZIEGO\": [\n",
    "        \"decyzja sędziego\", \"gwizdek rozpoczęcia\", \"gwizdek zakończenia\", \n",
    "        \"pokazanie kartki\", \"przyznanie rzutu karnego\", \"przyznanie rzutu wolnego\", \n",
    "        \"wznowienie gry\", \"przerwanie gry\", \"korzyść\", \"wideoweryfikacja VAR\", \n",
    "        \"anulowanie bramki\", \"przyznanie gola\", \"rzut sędziowski\"\n",
    "    ],\n",
    "    \"WARUNKI_ZEWNETRZNE\": [\n",
    "        \"warunki zewnętrzne\", \"stan murawy\", \"rodzaj nawierzchni\", \n",
    "        \"sztuczna murawa\", \"naturalna murawa\", \"system hybrydowy\", \n",
    "        \"warunki pogodowe\", \"oświetlenie stadionowe\", \"warunki widoczności\"\n",
    "    ],\n",
    "    \"ELEMENTY_WYDARZENIA\": [\n",
    "        \"element wydarzenia\", \"gol\", \"zdobycie bramki\", \"utrata bramki\", \n",
    "        \"interwencja bramkarza\", \"obrona strzału\", \"niecelny strzał\", \n",
    "        \"interwencja VAR\", \"zmiana zawodnika\", \"przewinienie taktyczne\", \n",
    "        \"kontuzja\", \"decyzja o dogrywce\"\n",
    "    ],\n",
    "    \"AKCJA\": [\n",
    "        \"wznowić grę\", \"rozpocząć grę\", \"przerwać grę\"\n",
    "    ],\n",
    "    \"LOKALIZACJA\": [\n",
    "        \"lokalizacja\", \"środek boiska\", \"prawa strona boiska\",\n",
    "        \"lewa strona boiska\", \"pole karne\", \"pole bramkowe\",\n",
    "        \"pole rożne\", \"środek pola karnego\", \"krawędź boiska\",\n",
    "        \"linia boczna\", \"linia bramkowa\", \"linia środkowa\",\n",
    "        \"kornik\", \"środek boiska\", \"strefa techniczna\",\n",
    "        \"trybuny\", \"środek punktu\", \"strefa kibica\",\n",
    "        \"strefa rozgrzewki\", \"strefa zmiany\", \"strefa VAR\"\n",
    "    ],\n",
    "    \"INFORMACJA\": [\n",
    "        \"informacja\", \"komunikat\", \"ogłoszenie\", \"zapowiedź\", \n",
    "        \"komunikat sędziowski\", \"komunikat organizatora\", \n",
    "        \"komunikat drużyny\", \"komunikat mediów\", \n",
    "        \"komunikat kibiców\", \"komunikat VAR\"\n",
    "    ],\n",
    "    \"KOMENTARZ\": [\n",
    "        \"komentarz\", \"komentarz meczowy\", \"analiza meczu\", \n",
    "        \"komentarz taktyczny\", \"komentarz sędziowski\", \n",
    "        \"komentarz ekspercki\", \"komentarz kibiców\", \n",
    "        \"komentarz mediów\", \"komentarz VAR\"\n",
    "    ],\n",
    "    \"STATYSTYKA\": [\n",
    "        \"statystyka\", \"posiadanie piłki\", \"liczba strzałów\", \n",
    "        \"liczba celnych strzałów\", \"liczba rzutów rożnych\", \n",
    "        \"liczba fauli\", \"liczba spalonych\", \"liczba interwencji bramkarza\", \n",
    "        \"liczba podań\", \"dokładność podań\", \"liczba odbiorów piłki\"\n",
    "    ],\n",
    "    \"KONTEKST\": [\n",
    "        \"kontekst\", \"sytuacja meczowa\", \"stan meczu\", \n",
    "        \"czas meczu\", \"wynik meczu\", \"przewaga drużyny\", \n",
    "        \"napięcie w grze\", \"emocje na boisku\", \n",
    "        \"reakcja kibiców\", \"reakcja mediów\"\n",
    "    ],\n",
    "    \"ROZMIAR\": [\n",
    "        \"rozmiar\", \"wielkość boiska\", \"wymiary boiska\", \n",
    "        \"długość boiska\", \"szerokość boiska\", \"wysokość bramki\", \n",
    "        \"szerokość bramki\", \"średnica piłki\", \"waga piłki\",\n",
    "        \"wymiary pola karnego\", \"wymiary pola bramkowego\", \n",
    "        \"wymiary pola rożnego\",\"maksymalne wymiary\", \n",
    "        \"minimalne wymiary\", \"standardowe wymiary\", \n",
    "        \"minimalne wymiary miedzynarodowe\",\n",
    "        \"maksymalne wymiary międzynarodowe\", \"wymiary boiska FIFA\",\n",
    "        \"przednia część\", \"tylna część\",\n",
    "        \"przednia krawędź\", \"tylna krawędź\",\n",
    "        \"tylna krawędź lini bramkowej\", \"przednia krawędź lini bramkowej\",\n",
    "    ],\n",
    "    \"RODZAJ\": [\n",
    "        \"FIFA\", \"UEFA\", \"PZPN\", \"liga\", \"puchar\",\n",
    "        \"mistrzostwa\", \"eliminacje\", \"kwalifikacje\",\n",
    "        \"turniej\", \"rozgrywki\", \"mecz towarzyski\",\n",
    "        \"mecz ligowy\", \"mecz pucharowy\", \"mecz eliminacyjny\",\n",
    "        \"mecz kwalifikacyjny\", \"mecz mistrzowski\",\n",
    "        \"mecz finałowy\", \"mecz grupowy\", \"mecz ćwierćfinałowy\",\n",
    "        \"krajowy\", \"międzynarodowy\", \"lokalny\",\n",
    "        \"klubowy\", \"reprezentacyjny\", \"mecz derbowy\",\n",
    "    ],\n",
    "    \"POLE GRY\": [\n",
    "        \"naturalne\", \"sztuczne\", \"hybrydowe\", \"trawa\",\n",
    "        \"murawa\", \"boisko\", \"pole gry\", \"nawierzchnia\",\n",
    "        \"nawierzchnia boiska\", \"nawierzchnia naturalna\",\n",
    "        \"nawierzchnia sztuczna\", \"nawierzchnia hybrydowa\",\n",
    "        \"nawierzchnia trawiasta\", \"nawierzchnia murawy\",\n",
    "        \"sztuczna trawa\", \"naturalna trawa\",\n",
    "        \n",
    "    ],\n",
    "    \"PUNKT RAPORTU\": [\n",
    "        \"punkt raportu\", \"punkt kontrolny\", \"punkt odniesienia\",\n",
    "        \"punkt pomiarowy\", \"punkt obserwacyjny\", \"punkt analizy\",\n",
    "        \"punkt weryfikacji\", \"punkt statystyczny\", \"punkt informacyjny\",\n",
    "        \"punkt lokalizacji\", \"punkt zdarzenia\", \"punkt akcji\",\n",
    "        \"punkt decyzji\", \"punkt sytuacji\", \"punkt kontekstu\",\n",
    "        \"punkt warunków zewnętrznych\", \"punkt 1\",\n",
    "        \"punkt 2\", \"punkt 3\", \"punkt 4\", \"punkt 5\", \"punkt 6\",\n",
    "        \"punkt 7\", \"punkt 8\", \"punkt 9\", \"punkt 10\",\n",
    "        \"ocena\", \"ocena sytuacji\", \"ocena decyzji\",\n",
    "    ]\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Namespace, RDF\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "g = Graph()\n",
    "g.parse(\"graf5s.rdf\", format=\"turtle\")\n",
    "\n",
    "for label, terms in entities.items():\n",
    "    patterns = [nlp.make_doc(term) for term in terms]\n",
    "    phrase_matcher.add(label, patterns)\n",
    "    for term in terms:\n",
    "        matcher.add(label, [[{\"LEMMA\": term}]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "def normalize_wordnet_term(term: str) -> str:\n",
    "    term = term.lower()\n",
    "    term = unidecode(term)\n",
    "    term = term.replace(\" \", \"_\")\n",
    "    term = re.sub(r\"[^\\w\\-]\", \"\", term)\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from rdflib import Graph, Namespace, URIRef, RDF, RDFS\n",
    "\n",
    "with open(\"plwordnet_4_2NOENGLISH_filtered.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    xml_data = f.read()\n",
    "    wrapped_xml = \"<PLWORDNET>\" + xml_data + \"</PLWORDNET>\"\n",
    "    \n",
    "root = ET.fromstring(wrapped_xml)\n",
    "\n",
    "synsets = {}\n",
    "relations = {}\n",
    "\n",
    "for syn in root.findall('.//SYNSET'):\n",
    "    sid = syn.findtext('ID')\n",
    "    literals = [lit.text for lit in syn.findall('./SYNONYM/LITERAL') if lit.text]\n",
    "    literals = [normalize_wordnet_term(l) for l in literals]\n",
    "    synsets[sid] = literals\n",
    "    for l in literals:\n",
    "        relations.setdefault(l, {\"synonyms\": set(), \"hyponyms\": set(), \"hypernyms\": set()})\n",
    "\n",
    "# Synonyms\n",
    "for literals in synsets.values():\n",
    "    for l in literals:\n",
    "        others = set(literals) - {l}\n",
    "        relations[l][\"synonyms\"].update(others)\n",
    "\n",
    "# Hyponyms / hypernyms\n",
    "for syn in root.findall('.//SYNSET'):\n",
    "    sid = syn.findtext('ID')\n",
    "    source_terms = synsets.get(sid, [])\n",
    "    for ilr in syn.findall('./ILR'):\n",
    "        target_sid = ilr.text.strip()\n",
    "        rel_type = ilr.findtext('TYPE')\n",
    "        target_terms = synsets.get(target_sid, [])\n",
    "\n",
    "        if not target_terms:\n",
    "            continue\n",
    "\n",
    "        for s in source_terms:\n",
    "            if rel_type.lower().startswith(\"hiponimia\"):\n",
    "                relations[s][\"hypernyms\"].update(target_terms)\n",
    "                for t in target_terms:\n",
    "                    relations[t][\"hyponyms\"].add(s)\n",
    "            elif rel_type.lower().startswith(\"hypernym\"):\n",
    "                relations[s][\"hyponyms\"].update(target_terms)\n",
    "                for t in target_terms:\n",
    "                    relations[t][\"hypernyms\"].add(s)\n",
    "\n",
    "# --- Load RDF graph ---\n",
    "rdf_words = set()\n",
    "for subj, pred, obj in g:\n",
    "    for node in (subj, obj):\n",
    "        if isinstance(node, URIRef):\n",
    "            name = str(node).split(\"/\")[-1]\n",
    "            rdf_words.add(name.lower())\n",
    "\n",
    "\n",
    "nlp_synonyms = stanza.Pipeline('pl', processors='tokenize,pos,lemma', verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "def replace_words_with_synonims(question: str) -> str:\n",
    "\n",
    "    doc = nlp_synonyms(question)\n",
    "    new_tokens = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            text = token.text\n",
    "            lemma = token.words[0].lemma\n",
    "            norm_lemma = normalize_wordnet_term(lemma)\n",
    "            repl = text\n",
    "\n",
    "            if norm_lemma in rdf_words or normalize_wordnet_term(text) in rdf_words:\n",
    "                new_tokens.append(text)\n",
    "                continue\n",
    "\n",
    "            best_match = None\n",
    "            if norm_lemma in relations:\n",
    "                candidates = (\n",
    "                    relations[norm_lemma][\"synonyms\"]\n",
    "                    | relations[norm_lemma][\"hyponyms\"]\n",
    "                    | relations[norm_lemma][\"hypernyms\"]\n",
    "                )\n",
    "                for c in candidates:\n",
    "                    if c in rdf_words:\n",
    "                        best_match = c\n",
    "                        break\n",
    "\n",
    "            if best_match:\n",
    "                repl = best_match\n",
    "                if text[0].isupper():\n",
    "                    repl = repl.capitalize()\n",
    "\n",
    "            new_tokens.append(repl)\n",
    "\n",
    "    result = \" \".join(new_tokens)\n",
    "    for punct in [',', '.', '?', '!', ':', ';']:\n",
    "        result = result.replace(' ' + punct, punct)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets_spacy(question: str, nlp, phrase_matcher, matcher):\n",
    "    doc = nlp(question)\n",
    "    matches = phrase_matcher(doc) + matcher(doc)\n",
    "    entity_spans = []\n",
    "    for _, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        span_lemma = \" \".join([token.lemma_.lower() for token in span])\n",
    "        entity_spans.append((start, end, span_lemma))\n",
    "    entity_spans = sorted(entity_spans, key=lambda x: (x[0], -(x[1]-x[0]), -len(x[2])))\n",
    "    final_spans = []\n",
    "    for start_i, end_i, ent_i in entity_spans:\n",
    "        overlap = False\n",
    "        for start_j, end_j, ent_j in final_spans:\n",
    "            if not (end_i <= start_j or start_i >= end_j):\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            final_spans.append((start_i, end_i, ent_i))\n",
    "    token_to_entity = {}\n",
    "    for start, end, ent in final_spans:\n",
    "        for i in range(start, end):\n",
    "            token_to_entity[i] = ent\n",
    "    IGNORED_NSUBJ = {\"kto\", \"co\", \"coś\", \"kogo\", \"czym\", \"który\", \"jaki\"}\n",
    "    MODAL_VERBS = {\"móc\", \"musieć\", \"chcieć\", \"potrafić\", \"powinien\", \"trzeba\", \"zezwalać\"}\n",
    "    triplets = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            verb = token.lemma_.lower()\n",
    "            children = list(token.children)\n",
    "            objects = []\n",
    "            for child in children:\n",
    "                if child.dep_ in (\"obj\", \"obl\"):\n",
    "                    obj_ent = token_to_entity.get(child.i, child.lemma_.lower())\n",
    "                    objects.append(obj_ent)\n",
    "            if not objects:\n",
    "                for child in children:\n",
    "                    if child.dep_ == \"nsubj\" and child.text.lower() not in IGNORED_NSUBJ:\n",
    "                        obj_ent = token_to_entity.get(child.i, child.lemma_.lower())\n",
    "                        objects.append(obj_ent)\n",
    "            for obj in objects:\n",
    "                triplets.append((verb, obj))\n",
    "            if verb in MODAL_VERBS:\n",
    "                for child in children:\n",
    "                    if child.pos_ == \"VERB\":\n",
    "                        main_verb = child.lemma_.lower()\n",
    "                        main_objects = []\n",
    "                        for tok in doc:\n",
    "                            if tok.dep_ in (\"obj\", \"obl\"):\n",
    "                                obj_ent = token_to_entity.get(tok.i, tok.lemma_.lower())\n",
    "                                main_objects.append(obj_ent)\n",
    "                        if not main_objects:\n",
    "                            for tok in doc:\n",
    "                                if tok.dep_ == \"nsubj\" and tok.text.lower() not in IGNORED_NSUBJ:\n",
    "                                    obj_ent = token_to_entity.get(tok.i, tok.lemma_.lower())\n",
    "                                    main_objects.append(obj_ent)\n",
    "                        for obj in main_objects:\n",
    "                            triplets.append((main_verb, obj))\n",
    "    triplets = list({(replace_words_with_synonims(v), replace_words_with_synonims(o)) for v, o in triplets if v and o})\n",
    "    return triplets\n",
    "\n",
    "def extract_triplets_stanza(question: str, nlp_stanza, entities):\n",
    "    doc = nlp_stanza(question)\n",
    "    lemmas = [w.lemma.lower() for sent in doc.sentences for w in sent.words]\n",
    "    found_entities = []\n",
    "    used = set()\n",
    "    for ent_text in sorted([e for v in entities.values() for e in v], key=lambda x: -len(x.split())):\n",
    "        ent_lemmas = ent_text.split()\n",
    "        for i in range(len(lemmas) - len(ent_lemmas) + 1):\n",
    "            if set(range(i, i+len(ent_lemmas))) & used:\n",
    "                continue\n",
    "            if lemmas[i:i+len(ent_lemmas)] == ent_lemmas:\n",
    "                found_entities.append((i, i+len(ent_lemmas), ent_text))\n",
    "                used.update(range(i, i+len(ent_lemmas)))\n",
    "    found_entities = sorted(found_entities, key=lambda x: x[0])\n",
    "    triplets = []\n",
    "    for sent in doc.sentences:\n",
    "        verbs = [w for w in sent.words if w.upos == 'VERB']\n",
    "        for verb in verbs:\n",
    "            verb_lemma = verb.lemma.lower()\n",
    "            subj = None\n",
    "            obj = None\n",
    "            for start, end, ent in found_entities:\n",
    "                if end <= verb.id - 1:\n",
    "                    subj = ent\n",
    "                elif start >= verb.id:\n",
    "                    obj = ent\n",
    "                    break\n",
    "            if subj and obj:\n",
    "                triplets.append((verb_lemma, obj))\n",
    "            elif obj:\n",
    "                triplets.append((verb_lemma, obj))\n",
    "            elif subj:\n",
    "                triplets.append((verb_lemma, subj))\n",
    "    triplets = list({(replace_words_with_synonims(v), replace_words_with_synonims(o)) for v, o in triplets if v and o})\n",
    "    return triplets\n",
    "\n",
    "def build_sparql_query_from_question(question: str, graph: Graph, method: str = \"spacy\"):\n",
    "    if method == \"stanza\":\n",
    "        triplets = extract_triplets_stanza(question, nlp_stanza, entities)\n",
    "    else:\n",
    "        triplets = extract_triplets_spacy(question, nlp, phrase_matcher, matcher)\n",
    "    sparql_queries = []\n",
    "    sparql_prefix = (\n",
    "        \"PREFIX ex: <http://example.org/>\\n\"\n",
    "        \"SELECT DISTINCT ?subject ?predicate ?object WHERE {\\n\"\n",
    "    )\n",
    "\n",
    "    for verb, obj in triplets:\n",
    "        for neg in [False, True]:\n",
    "            if neg:\n",
    "                verb_mod = f\"nie_{verb}\" if not verb.startswith(\"nie_\") else verb\n",
    "            else:\n",
    "                verb_mod = verb.replace(\"nie_\", \"\") if verb.startswith(\"nie_\") else verb\n",
    "            verb_uri = f\"ex:{clean_for_uri(verb_mod)}\"\n",
    "            obj_uri = f\"ex:{clean_for_uri(obj)}\"\n",
    "            sparql_body = f\"  {{\\n      ?subject {verb_uri} {obj_uri} .\\n      BIND({verb_uri} AS ?predicate)\\n      BIND({obj_uri} AS ?object)\\n  }}\\n  UNION\\n  {{\\n      ?subject {verb_uri} ?object .\\n      FILTER(CONTAINS(LCASE(STR(?object)), \\\"{obj.lower()}\\\")) .\\n      BIND({verb_uri} AS ?predicate)\\n  }}\\n\"\n",
    "            sparql_query = sparql_prefix + sparql_body + \"}\"\n",
    "            sparql_queries.append(sparql_query)\n",
    "\n",
    "            sparql_body_rev = f\"  {{\\n      {obj_uri} {verb_uri} ?subject .\\n      BIND({verb_uri} AS ?predicate)\\n      BIND({obj_uri} AS ?object)\\n  }}\\n  UNION\\n  {{\\n      {obj_uri} {verb_uri} ?object .\\n      FILTER(CONTAINS(LCASE(STR(?object)), \\\"{verb_mod.lower()}\\\")) .\\n      BIND({verb_uri} AS ?predicate)\\n  }}\\n\"\n",
    "            sparql_query_rev = sparql_prefix + sparql_body_rev + \"}\"\n",
    "            sparql_queries.append(sparql_query_rev)\n",
    "\n",
    "    if len(triplets) >= 2:\n",
    "        for i in range(len(triplets)):\n",
    "            for j in range(len(triplets)):\n",
    "                if i != j:\n",
    "                    for neg1 in [False, True]:\n",
    "                        for neg2 in [False, True]:\n",
    "                            verb1, obj1 = triplets[i]\n",
    "                            verb2, obj2 = triplets[j]\n",
    "                            if neg1:\n",
    "                                verb1_mod = f\"nie_{verb1}\" if not verb1.startswith(\"nie_\") else verb1\n",
    "                            else:\n",
    "                                verb1_mod = verb1.replace(\"nie_\", \"\") if verb1.startswith(\"nie_\") else verb1\n",
    "                            if neg2:\n",
    "                                verb2_mod = f\"nie_{verb2}\" if not verb2.startswith(\"nie_\") else verb2\n",
    "                            else:\n",
    "                                verb2_mod = verb2.replace(\"nie_\", \"\") if verb2.startswith(\"nie_\") else verb2\n",
    "                            verb1_uri = f\"ex:{clean_for_uri(verb1_mod)}\"\n",
    "                            obj1_uri = f\"ex:{clean_for_uri(obj1)}\"\n",
    "                            verb2_uri = f\"ex:{clean_for_uri(verb2_mod)}\"\n",
    "                            obj2_uri = f\"ex:{clean_for_uri(obj2)}\"\n",
    "\n",
    "                            sparql_body_chain = (\n",
    "                                f\"  ?subject {verb1_uri} {obj1_uri} .\\n\"\n",
    "                                f\"  {obj1_uri} {verb2_uri} {obj2_uri} .\\n\"\n",
    "                                f\"  BIND({verb1_uri} AS ?predicate)\\n\"\n",
    "                                f\"  BIND({verb2_uri} AS ?predicate2)\\n\"\n",
    "                                f\"  BIND({obj2_uri} AS ?object)\\n\"\n",
    "                            )\n",
    "                            sparql_query_chain = sparql_prefix + sparql_body_chain + \"}\"\n",
    "                            sparql_queries.append(sparql_query_chain)\n",
    "\n",
    "    if len(triplets) >= 2:\n",
    "        for i in range(len(triplets)):\n",
    "            for j in range(len(triplets)):\n",
    "                if i != j:\n",
    "                    for neg1 in [False, True]:\n",
    "                        for neg2 in [False, True]:\n",
    "                            verb1, obj1 = triplets[i]\n",
    "                            verb2, obj2 = triplets[j]\n",
    "                            if neg1:\n",
    "                                verb1_mod = f\"nie_{verb1}\" if not verb1.startswith(\"nie_\") else verb1\n",
    "                            else:\n",
    "                                verb1_mod = verb1.replace(\"nie_\", \"\") if verb1.startswith(\"nie_\") else verb1\n",
    "                            if neg2:\n",
    "                                verb2_mod = f\"nie_{verb2}\" if not verb2.startswith(\"nie_\") else verb2\n",
    "                            else:\n",
    "                                verb2_mod = verb2.replace(\"nie_\", \"\") if verb2.startswith(\"nie_\") else verb2\n",
    "                            verb1_uri = f\"ex:{clean_for_uri(verb1_mod)}\"\n",
    "                            verb2_uri = f\"ex:{clean_for_uri(verb2_mod)}\"\n",
    "                            obj2_uri = f\"ex:{clean_for_uri(obj2)}\"\n",
    "\n",
    "                            sparql_body_path = (\n",
    "                                f\"  ?subject {verb1_uri}/{verb2_uri} {obj2_uri} .\\n\"\n",
    "                                f\"  BIND({verb1_uri} AS ?predicate)\\n\"\n",
    "                                f\"  BIND({verb2_uri} AS ?predicate2)\\n\"\n",
    "                                f\"  BIND({obj2_uri} AS ?object)\\n\"\n",
    "                            )\n",
    "                            sparql_query_path = sparql_prefix + sparql_body_path + \"}\"\n",
    "                            sparql_queries.append(sparql_query_path)\n",
    "                            \n",
    "                            sparql_body_path_rev = (\n",
    "                                f\"  ?subject ^{verb1_uri}/^{verb2_uri} {obj2_uri} .\\n\"\n",
    "                                f\"  BIND({verb1_uri} AS ?predicate)\\n\"\n",
    "                                f\"  BIND({verb2_uri} AS ?predicate2)\\n\"\n",
    "                                f\"  BIND({obj2_uri} AS ?object)\\n\"\n",
    "                            )\n",
    "                            sparql_query_path_rev = sparql_prefix + sparql_body_path_rev + \"}\"\n",
    "                            sparql_queries.append(sparql_query_path_rev)\n",
    "    return sparql_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Czy obserwator musi odjąć od oceny 0,1 jeżeli błąd sędziego co do udzielenia bądź nieudzielenia napomnienia nie był ewidentny, a sytuacja była przedmiotem pomeczowej rozmowy/analizy?\"\n",
    "rewritten_question = replace_words_with_synonims(question)\n",
    "sparql = build_sparql_query_from_question(rewritten_question, g)\n",
    "print(sparql)\n",
    "\n",
    "\n",
    "def return_query(question: str) -> str:\n",
    "    sparql_list_spacy = build_sparql_query_from_question(question, g, 'spacy')\n",
    "    sparql_list_stanza = build_sparql_query_from_question(question, g, 'stanza')\n",
    "\n",
    "    seen = set()\n",
    "    sparql_list = []\n",
    "    for q in sparql_list_spacy + sparql_list_stanza:\n",
    "        if q not in seen:\n",
    "            sparql_list.append(q)\n",
    "            seen.add(q)\n",
    "\n",
    "    trios = []\n",
    "    for idx, sparql in enumerate(sparql_list, 1):\n",
    "        qres = g.query(sparql)\n",
    "        for row in qres:\n",
    "            subj = row.subject.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "            pred = row.predicate.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "            obj = row.object.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "            \n",
    "            if None in (subj, pred, obj):\n",
    "                print(\"Niepełne dane w trójce, pomijam wynik.\")\n",
    "                continue\n",
    "            \n",
    "            trios.append((subj, pred, obj))\n",
    "\n",
    "    return trios\n",
    "\n",
    "def return_query_with_synonims(question: str) -> str:\n",
    "    sparql_list_spacy = build_sparql_query_from_question(question, g, 'spacy')\n",
    "    sparql_list_stanza = build_sparql_query_from_question(question, g, 'stanza')\n",
    "\n",
    "    seen = set()\n",
    "    sparql_list = []\n",
    "    for q in sparql_list_spacy + sparql_list_stanza:\n",
    "        if q not in seen:\n",
    "            sparql_list.append(q)\n",
    "            seen.add(q)\n",
    "\n",
    "    trios = []\n",
    "    for idx, sparql in enumerate(sparql_list, 1):\n",
    "        qres = g.query(sparql)\n",
    "        for row in qres:\n",
    "            subj = row.subject.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "            pred = row.predicate.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "            obj = row.object.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "            \n",
    "            if None in (subj, pred, obj):\n",
    "                print(\"Niepełne dane w trójce, pomijam wynik.\")\n",
    "                continue\n",
    "            \n",
    "            trios.append((subj, pred, obj))\n",
    "\n",
    "    return trios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Piłka całym obwodem opuściła boisko przez linię boczną. Sędzia asystent zasygnalizował sędziemu że gra powinna zostać wznowiona wrzutem. Czy słusznie?\"\n",
    "sparql_list_spacy = build_sparql_query_from_question(question, g, 'spacy')\n",
    "sparql_list_stanza = build_sparql_query_from_question(question, g, 'stanza')\n",
    "\n",
    "seen = set()\n",
    "sparql_list = []\n",
    "for q in sparql_list_spacy + sparql_list_stanza:\n",
    "    if q not in seen:\n",
    "        sparql_list.append(q)\n",
    "        seen.add(q)\n",
    "\n",
    "all_triplets = []\n",
    "for idx, sparql_list in enumerate(sparql_list, 1):\n",
    "    print(question)\n",
    "    print(sparql_list)\n",
    "    print(\"Odpowiedzi:\")\n",
    "    qres = g.query(sparql_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Eksperyment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Nie znaleziono zmiennej środowiskowej GOOGLE_API_KEY. \"\n",
    "                         \"Ustaw ją lub skonfiguruj klucz bezpośrednio w kodzie.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "except Exception as e:\n",
    "    print(f\"Błąd konfiguracji API: {e}\")\n",
    "    exit()\n",
    "\n",
    "generation_config = genai.types.GenerationConfig(\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Eksperyment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import csv\n",
    "\n",
    "questions = [\n",
    "    \"Czy punkt karny wyznacza się ze środka punktu do przedniej krawędzi linii bramkowej?\",\n",
    "    \"Czy punkt karny wyznacza się ze środka punktu do tylnej krawędzi linii bramkowej?\",\n",
    "    \"Czy zezwala się na zastosowanie sztucznej trawy, na naturalnej nawierzchni, w celu wykonania oznaczeń pola gry, zakładając, że oznaczenia takie nie stanowią zagrożenia?\",\n",
    "    \"Czy przepisy gry określają maksymalną szerokość linii na polu gry?\",\n",
    "    \"Czy w przypadku oceny niższej niż 7,7 obserwator musi omówić z sędziami elementy do poprawy podczas pomeczowej rozmowy, a następnie zanotować je w raporcie w punkcie 7 („Uwagi o charakterze ogólnym…”) i punkcie 8 („Punkty omówione z sędzią”) oraz w punktach 9 i 10 (dotyczy sędziów asystentów)?\",\n",
    "    \"Czy bramka może zostać zdobyta bezpośrednio z wrzutu z autu?\",\n",
    "    \"Czy za zagranie piłki ręką w polu karnym przez zawodnika drużyny broniącej sędzia może podyktować rzut wolny pośredni?\",\n",
    "    \"Czy uprawniony do wykonywania rzutów karnych zawodnik może zmienić się funkcją z bramkarzem?\",\n",
    "    \"Czy drużyna która przegrała losowanie zawsze rozpoczyna grę?\",\n",
    "    \"Piłka całym obwodem opuściła boisko przez linię boczną. Sędzia asystent zasygnalizował sędziemu że gra powinna zostać wznowiona wrzutem. Czy słusznie?\"\n",
    "]\n",
    "\n",
    "MODELS = {\n",
    "    \"chatgpt4\": lambda prompt: query_chatgpt4(prompt),\n",
    "    \"gemini_1_5\": lambda prompt: query_gemini1_5(prompt),\n",
    "    \"chatgpt_3_5\": lambda prompt: query_chatgpt3_5(prompt),\n",
    "    \"gemma\": lambda prompt: query_gemma(prompt),\n",
    "}\n",
    "\n",
    "def create_prompt(question: str) -> str:\n",
    "    prompt = (\n",
    "        f'Odpowiedz na następujące pytanie: \"{question}\"\\n'\n",
    "        \"Odpowiedz zwięźle i precyzyjnie.\"\n",
    "        \"Odpowiedz w języku polskim, używając maksymalnie 1-2 zdań.\"\n",
    "    )\n",
    "    return prompt.strip()\n",
    "\n",
    "def query_chatgpt4(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"ERROR querying GPT-4\"\n",
    "\n",
    "def query_gemini1_5(prompt: str) -> str:\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "\n",
    "        if response.parts:\n",
    "            text_temp = []\n",
    "            for part in response.parts:\n",
    "                text_temp.append(part.text)\n",
    "            return \" \".join(text_temp)\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while communicating with the Gemini API: {e}\")\n",
    "    return \"ERROR querying gemini-1.5\"\n",
    "\n",
    "def query_chatgpt3_5(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"ERROR querying GPT-3.5\"\n",
    "\n",
    "def query_gemma(prompt: str) -> str:\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemma-3n-e4b-it')\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "\n",
    "        if response.parts:\n",
    "            text_temp = []\n",
    "            for part in response.parts:\n",
    "                text_temp.append(part.text)\n",
    "            return \" \".join(text_temp)\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while communicating with the Gemini API: {e}\")\n",
    "    return \"Error querying Gemma\"\n",
    "\n",
    "def run_experiment(questions: List[str], models: Dict[str, callable]):\n",
    "    results = []\n",
    "\n",
    "    for model_name, model_fn in models.items():\n",
    "        print(f\"Running model: '{model_name}'\")\n",
    "        for question in questions:\n",
    "            try:\n",
    "                prompt = create_prompt(question)\n",
    "                answer = model_fn(prompt)\n",
    "\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with question '{question}' and model '{model_name}': {e}\")\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": f\"Error: {str(e)}\"\n",
    "                })\n",
    "\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "    with open(\"./results/ex1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    csv_header = [\"model\"] + [f\"Q{i+1}\" for i in range(len(questions))]\n",
    "    csv_rows = {}\n",
    "\n",
    "    for entry in results:\n",
    "        model = entry[\"model\"]\n",
    "        question_index = questions.index(entry[\"question\"])\n",
    "        answer = entry[\"answer\"]\n",
    "\n",
    "        if model not in csv_rows:\n",
    "            csv_rows[model] = [\"\"] * len(questions)\n",
    "\n",
    "        csv_rows[model][question_index] = answer\n",
    "\n",
    "    with open(\"./results/ex1.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=';' ,quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(csv_header)\n",
    "        for model, answers in csv_rows.items():\n",
    "            writer.writerow([model] + answers)\n",
    "\n",
    "    print(\"Experiment completed and saved to results.json and results.csv\")\n",
    "\n",
    "run_experiment(questions, MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Eksperyment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import csv\n",
    "\n",
    "questions = [\n",
    "    \"Czy punkt karny wyznacza się ze środka punktu do przedniej krawędzi linii bramkowej?\",\n",
    "    \"Czy punkt karny wyznacza się ze środka punktu do tylnej krawędzi linii bramkowej?\",\n",
    "    \"Czy zezwala się na zastosowanie sztucznej trawy, na naturalnej nawierzchni, w celu wykonania oznaczeń pola gry, zakładając, że oznaczenia takie nie stanowią zagrożenia?\",\n",
    "    \"Czy przepisy gry określają maksymalną szerokość linii na polu gry?\",\n",
    "    \"Czy w przypadku oceny niższej niż 7,7 obserwator musi omówić z sędziami elementy do poprawy podczas pomeczowej rozmowy, a następnie zanotować je w raporcie w punkcie 7 („Uwagi o charakterze ogólnym…”) i punkcie 8 („Punkty omówione z sędzią”) oraz w punktach 9 i 10 (dotyczy sędziów asystentów)?\",\n",
    "    \"Czy bramka może zostać zdobyta bezpośrednio z wrzutu z autu?\",\n",
    "    \"Czy za zagranie piłki ręką w polu karnym przez zawodnika drużyny broniącej sędzia może podyktować rzut wolny pośredni?\",\n",
    "    \"Czy uprawniony do wykonywania rzutów karnych zawodnik może zmienić się funkcją z bramkarzem?\",\n",
    "    \"Czy drużyna która przegrała losowanie zawsze rozpoczyna grę?\",\n",
    "    \"Piłka całym obwodem opuściła boisko przez linię boczną. Sędzia asystent zasygnalizował sędziemu że gra powinna zostać wznowiona wrzutem. Czy słusznie?\"\n",
    "]\n",
    "\n",
    "MODELS = {\n",
    "    \"chatgpt4\": lambda prompt: query_chatgpt4(prompt),\n",
    "    \"gemini_1_5\": lambda prompt: query_gemini1_5(prompt),\n",
    "    \"chatgpt_3_5\": lambda prompt: query_chatgpt3_5(prompt),\n",
    "    \"gemma\": lambda prompt: query_gemma(prompt),\n",
    "}\n",
    "\n",
    "def create_prompt(question: str, fact: str) -> str:\n",
    "    prompt = (\n",
    "        f'Na podstawie trójek faktów z grafu wiedzy (każda trójka zapisana jest w nawiasie): \"{fact}\"\\n'\n",
    "        f'Odpowiedz na następujące pytanie: \"{question}\"\\n'\n",
    "        \"Odpowiedz zwięźle i precyzyjnie.\"\n",
    "        \"Odpowiedz w języku polskim, używając maksymalnie 1-2 zdań.\"\n",
    "    )\n",
    "    return prompt.strip()\n",
    "\n",
    "def query_chatgpt4(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"ERROR querying GPT-4\"\n",
    "\n",
    "def query_gemini1_5(prompt: str) -> str:\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "\n",
    "        if response.parts:\n",
    "            text_temp = []\n",
    "            for part in response.parts:\n",
    "                text_temp.append(part.text)\n",
    "            return \" \".join(text_temp)\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while communicating with the Gemini API: {e}\")\n",
    "    return \"ERROR querying gemini-1.5\"\n",
    "\n",
    "def query_chatgpt3_5(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"ERROR querying GPT-3.5\"\n",
    "\n",
    "def query_gemma(prompt: str) -> str:\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemma-3n-e4b-it')\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "\n",
    "        if response.parts:\n",
    "            text_temp = []\n",
    "            for part in response.parts:\n",
    "                text_temp.append(part.text)\n",
    "            return \" \".join(text_temp)\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while communicating with the Gemini API: {e}\")\n",
    "    return \"Error querying Gemma\"\n",
    "\n",
    "def run_experiment(questions: List[str], models: Dict[str, callable]):\n",
    "    results = []\n",
    "\n",
    "    for model_name, model_fn in models.items():\n",
    "        print(f\"Running model: '{model_name}'\")\n",
    "        for question in questions:\n",
    "            try:\n",
    "                trios = return_query(question)\n",
    "                fact = \", \".join([f\"({subj}, {pred}, {obj})\" for subj, pred, obj in trios])\n",
    "                prompt = create_prompt(question, fact)\n",
    "                answer = model_fn(prompt)\n",
    "\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"question\": question,\n",
    "                    \"fact\": fact,\n",
    "                    \"answer\": answer\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with question '{question}' and model '{model_name}': {e}\")\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"question\": question,\n",
    "                    \"fact\": None,\n",
    "                    \"answer\": f\"Error: {str(e)}\"\n",
    "                })\n",
    "\n",
    "\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "    with open(\"./results/ex2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    csv_header = [\"model\"] + [f\"Q{i+1}\" for i in range(len(questions))]\n",
    "    csv_rows = {}\n",
    "\n",
    "    for entry in results:\n",
    "        model = entry[\"model\"]\n",
    "        question_index = questions.index(entry[\"question\"])\n",
    "        answer = entry[\"answer\"]\n",
    "\n",
    "        if model not in csv_rows:\n",
    "            csv_rows[model] = [\"\"] * len(questions)\n",
    "\n",
    "        csv_rows[model][question_index] = answer\n",
    "\n",
    "    with open(\"./results/ex2.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=';' ,quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(csv_header)\n",
    "        for model, answers in csv_rows.items():\n",
    "            writer.writerow([model] + answers)\n",
    "\n",
    "    print(\"Experiment completed and saved to results.json and results.csv\")\n",
    "\n",
    "run_experiment(questions, MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Eksperyment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import csv\n",
    "\n",
    "questions = [\n",
    "    \"Czy punkt karny wyznacza się ze środka punktu do przedniej krawędzi linii bramkowej?\",\n",
    "    \"Czy punkt karny wyznacza się ze środka punktu do tylnej krawędzi linii bramkowej?\",\n",
    "    \"Czy zezwala się na zastosowanie sztucznej trawy, na naturalnej nawierzchni, w celu wykonania oznaczeń pola gry, zakładając, że oznaczenia takie nie stanowią zagrożenia?\",\n",
    "    \"Czy przepisy gry określają maksymalną szerokość linii na polu gry?\",\n",
    "    \"Czy w przypadku oceny niższej niż 7,7 obserwator musi omówić z sędziami elementy do poprawy podczas pomeczowej rozmowy, a następnie zanotować je w raporcie w punkcie 7 („Uwagi o charakterze ogólnym…”) i punkcie 8 („Punkty omówione z sędzią”) oraz w punktach 9 i 10 (dotyczy sędziów asystentów)?\",\n",
    "    \"Czy bramka może zostać zdobyta bezpośrednio z wrzutu z autu?\",\n",
    "    \"Czy za zagranie piłki ręką w polu karnym przez zawodnika drużyny broniącej sędzia może podyktować rzut wolny pośredni?\",\n",
    "    \"Czy uprawniony do wykonywania rzutów karnych zawodnik może zmienić się funkcją z bramkarzem?\",\n",
    "    \"Czy drużyna która przegrała losowanie zawsze rozpoczyna grę?\",\n",
    "    \"Piłka całym obwodem opuściła boisko przez linię boczną. Sędzia asystent zasygnalizował sędziemu że gra powinna zostać wznowiona wrzutem. Czy słusznie?\"\n",
    "]\n",
    "\n",
    "MODELS = {\n",
    "    \"chatgpt4\": lambda prompt: query_chatgpt4(prompt),\n",
    "    \"gemini_1_5\": lambda prompt: query_gemini1_5(prompt),\n",
    "    \"chatgpt_3_5\": lambda prompt: query_chatgpt3_5(prompt),\n",
    "    \"gemma\": lambda prompt: query_gemma(prompt),\n",
    "}\n",
    "\n",
    "def create_prompt(question: str, fact: str) -> str:\n",
    "    prompt = (\n",
    "        f'Na podstawie trójek faktów z grafu wiedzy (każda trójka zapisana jest w nawiasie): \"{fact}\"\\n'\n",
    "        f'Odpowiedz na następujące pytanie: \"{question}\"\\n'\n",
    "        \"Odpowiedz zwięźle i precyzyjnie.\"\n",
    "        \"Odpowiedz w języku polskim, używając maksymalnie 1-2 zdań.\"\n",
    "    )\n",
    "    return prompt.strip()\n",
    "\n",
    "def query_chatgpt4(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"ERROR querying GPT-4\"\n",
    "\n",
    "def query_gemini1_5(prompt: str) -> str:\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "\n",
    "        if response.parts:\n",
    "            text_temp = []\n",
    "            for part in response.parts:\n",
    "                text_temp.append(part.text)\n",
    "            return \" \".join(text_temp)\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while communicating with the Gemini API: {e}\")\n",
    "    return \"ERROR querying gemini-1.5\"\n",
    "\n",
    "def query_chatgpt3_5(prompt: str) -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return \"ERROR querying GPT-3.5\"\n",
    "\n",
    "def query_gemma(prompt: str) -> str:\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemma-3n-e4b-it')\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "\n",
    "        if response.parts:\n",
    "            text_temp = []\n",
    "            for part in response.parts:\n",
    "                text_temp.append(part.text)\n",
    "            return \" \".join(text_temp)\n",
    "        else:\n",
    "            return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while communicating with the Gemini API: {e}\")\n",
    "    return \"Error querying Gemma\"\n",
    "\n",
    "def run_experiment(questions: List[str], models: Dict[str, callable]):\n",
    "    results = []\n",
    "\n",
    "    for model_name, model_fn in models.items():\n",
    "        print(f\"Running model: '{model_name}'\")\n",
    "        for question in questions:\n",
    "            try:\n",
    "                trios = return_query_with_synonims(question)\n",
    "                fact = \", \".join([f\"({subj}, {pred}, {obj})\" for subj, pred, obj in trios])\n",
    "                prompt = create_prompt(question, fact)\n",
    "                answer = model_fn(prompt)\n",
    "\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"question\": question,\n",
    "                    \"fact\": fact,\n",
    "                    \"answer\": answer\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error with question '{question}' and model '{model_name}': {e}\")\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"question\": question,\n",
    "                    \"fact\": None,\n",
    "                    \"answer\": f\"Error: {str(e)}\"\n",
    "                })\n",
    "\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "    with open(\"./results/ex3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    csv_header = [\"model\"] + [f\"Q{i+1}\" for i in range(len(questions))]\n",
    "    csv_rows = {}\n",
    "\n",
    "    for entry in results:\n",
    "        model = entry[\"model\"]\n",
    "        question_index = questions.index(entry[\"question\"])\n",
    "        answer = entry[\"answer\"]\n",
    "\n",
    "        if model not in csv_rows:\n",
    "            csv_rows[model] = [\"\"] * len(questions)\n",
    "\n",
    "        csv_rows[model][question_index] = answer\n",
    "\n",
    "    with open(\"./results/ex3.csv\", \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=';' ,quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(csv_header)\n",
    "        for model, answers in csv_rows.items():\n",
    "            writer.writerow([model] + answers)\n",
    "\n",
    "    print(\"Experiment completed and saved to results.json and results.csv\")\n",
    "\n",
    "run_experiment(questions, MODELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
