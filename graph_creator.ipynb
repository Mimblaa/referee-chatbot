{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9dcfe1a",
   "metadata": {},
   "source": [
    "# PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a818f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Duch gry – filozofia  Przepisów  Piłka nożna jest najpiękniejszym sportem na świecie. Uprawi\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, start_page=12):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate starting from the given page (start_page - 1, it skips title page and list of contents)\n",
    "    for page_num in range(start_page - 1, len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text += page.get_text(\"text\").replace(\"\\n\", \" \")\n",
    "    \n",
    "    return text\n",
    "\n",
    "pdf_path = \"dane/PrzepisyGry_2024_25.pdf\"\n",
    "extracted_text1 = extract_text_from_pdf(pdf_path, start_page=12)\n",
    "\n",
    "# Display the first 100 characters of the text\n",
    "print(extracted_text1[:100])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa3e17",
   "metadata": {},
   "source": [
    "# Load Additional Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary wykonywane są od zewnętrznej krawędzi linii, jako że linie te należą do powierzchni, których\n"
     ]
    }
   ],
   "source": [
    "with open(\"dane/explanations.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    extracted_text2= f.read()\n",
    "print(extracted_text2[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1732103",
   "metadata": {},
   "source": [
    "# Combine Extracted Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = extracted_text1 + \"\\n\" + extracted_text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655d674",
   "metadata": {},
   "source": [
    "## URI Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "def clean_for_uri(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[ąćęłńóśźż]\", lambda m: {\n",
    "        'ą': 'a', 'ć': 'c', 'ę': 'e', 'ł': 'l',\n",
    "        'ń': 'n', 'ó': 'o', 'ś': 's', 'ź': 'z', 'ż': 'z'\n",
    "    }[m.group()], text)\n",
    "    text = text.replace(\" \", \"_\")\n",
    "    text = re.sub(r\"[^\\w\\-]\", \"\", text)\n",
    "    # NOTE: here we use urllib.parse.quote, which encodes to URI-safe format\n",
    "    return urllib.parse.quote(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5818d58",
   "metadata": {},
   "source": [
    "## Entity Dictionary\n",
    "This section defines an extended dictionary of entities relevant to football (soccer) rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4476718",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {\n",
    "    \"LINIA\": [\n",
    "        \"linia\", \"linia boczna\", \"linia bramkowa\", \"linia środkowa\", \n",
    "        \"punkt\", \"punkt karny\", \"punkt środkowy\", \n",
    "        \"linia pola karnego\", \"linia pola bramkowego\", \"linia pola rożnego\", \n",
    "        \"łuk pola karnego\", \"łuk pola rożnego\", \"linia spalonego\"\n",
    "    ],\n",
    "    \"POZYCJA\": [\n",
    "        \"zawodnik\", \"bramkarz\", \"obrońca\", \"środkowy obrońca\", \"boczny obrońca\",\n",
    "        \"pomocnik\", \"pomocnik defensywny\", \"pomocnik ofensywny\", \"skrzydłowy\",\n",
    "        \"napastnik\", \"rezerwowy\", \"zawodnik wymieniony\", \"kapitan drużyny\",\n",
    "        \"trener\", \"asystent trenera\", \"sędzia\", \"sędzia główny\", \n",
    "        \"sędzia asystent\", \"sędzia techniczny\", \"sędzia VAR\", \n",
    "        \"operator VAR\", \"lekarz drużyny\", \"fizjoterapeuta\", \"kierownik drużyny\"\n",
    "    ],\n",
    "    \"SPRZĘT\": [\n",
    "        \"sprzęt\", \"piłka\", \"piłka meczowa\", \"buty\", \"korki\", \"strój\", \n",
    "        \"getry\", \"ochraniacze\", \"ochraniacze goleni\", \"rękawice bramkarskie\",\n",
    "        \"chorągiewka\", \"chorągiewka boczna\", \"bramka\", \"siatka\", \n",
    "        \"poprzeczka\", \"słupek\", \"tablica zmian\", \"gwizdek\", \"zegarek sędziego\", \n",
    "        \"kamera VAR\", \"monitor VAR\", \"system GLT\", \"system VAR\"\n",
    "    ],\n",
    "    \"CZYNNOŚĆ\": [\n",
    "        \"czynność\", \"rozpoczęcie gry\", \"wznowienie gry\", \"rzut karny\", \n",
    "        \"rzut wolny\", \"rzut wolny pośredni\", \"rzut wolny bezpośredni\", \n",
    "        \"rzut z autu\", \"rzut rożny\", \"rzut od bramki\", \"dogrywka\", \n",
    "        \"rzuty karne\", \"wrzut\", \"drybling\", \"pressing\", \"odbiór piłki\", \n",
    "        \"strzał\", \"strzał na bramkę\", \"podanie\", \"asysta\", \"zagranie ręką\",\n",
    "        \"przewinienie\", \"przewinienie taktyczne\", \"symulacja\", \"opóźnianie gry\",\n",
    "        \"brutalność\", \"gra niebezpieczna\", \"korzyść sędziowska\", \"wideoweryfikacja\"\n",
    "    ],\n",
    "    \"CZAS\": [\n",
    "        \"czas\", \"czas gry\", \"pierwsza połowa\", \"druga połowa\", \n",
    "        \"doliczony czas\", \"doliczony czas gry\", \"dogrywka\", \"rzuty karne\", \n",
    "        \"seria rzutów karnych\", \"przerwa\", \"przerwa w grze\", \n",
    "        \"przerwa na chłodzenie\", \"czas zawieszenia\", \"czas przewinienia\"\n",
    "    ],\n",
    "    \"ORGANIZACJA\": [\n",
    "        \"organizacja\", \"IFAB\", \"FIFA\", \"UEFA\", \"PZPN\", \"konfederacja\", \n",
    "        \"federacja krajowa\", \"komisja sędziowska\", \"organizator rozgrywek\"\n",
    "    ],\n",
    "    \"SANKCJA\": [\n",
    "        \"sankcja\", \"żółta kartka\", \"czerwona kartka\", \"napomnienie\", \n",
    "        \"wykluczenie\", \"kara wychowawcza\", \"zawieszenie czasowe\", \n",
    "        \"zawieszenie meczowe\", \"kara techniczna\", \"upomnienie słowne\"\n",
    "    ],\n",
    "    \"OSOBA_FUNCKYJNA\": [\n",
    "        \"osoba funkcyjna\", \"trener\", \"asystent trenera\", \"fizjoterapeuta\", \n",
    "        \"lekarz drużyny\", \"analityk wideo\", \"kierownik drużyny\", \n",
    "        \"sędzia techniczny\", \"delegat meczu\", \"operator VAR\"\n",
    "    ],\n",
    "    \"STAN_GRY\": [\n",
    "        \"stan gry\", \"spalony\", \"przewinienie\", \"faul\", \"symulacja\", \n",
    "        \"opóźnianie gry\", \"brutalność\", \"gra niebezpieczna\", \n",
    "        \"korzyść sędziowska\", \"nieuznana bramka\", \"kontuzja zawodnika\"\n",
    "    ],\n",
    "    \"DECYZJA_SEDZIEGO\": [\n",
    "        \"decyzja sędziego\", \"gwizdek rozpoczęcia\", \"gwizdek zakończenia\", \n",
    "        \"pokazanie kartki\", \"przyznanie rzutu karnego\", \"przyznanie rzutu wolnego\", \n",
    "        \"wznowienie gry\", \"przerwanie gry\", \"korzyść\", \"wideoweryfikacja VAR\", \n",
    "        \"anulowanie bramki\", \"przyznanie gola\", \"rzut sędziowski\"\n",
    "    ],\n",
    "    \"WARUNKI_ZEWNETRZNE\": [\n",
    "        \"warunki zewnętrzne\", \"stan murawy\", \"rodzaj nawierzchni\", \n",
    "        \"sztuczna murawa\", \"naturalna murawa\", \"system hybrydowy\", \n",
    "        \"warunki pogodowe\", \"oświetlenie stadionowe\", \"warunki widoczności\"\n",
    "    ],\n",
    "    \"ELEMENTY_WYDARZENIA\": [\n",
    "        \"element wydarzenia\", \"gol\", \"zdobycie bramki\", \"utrata bramki\", \n",
    "        \"interwencja bramkarza\", \"obrona strzału\", \"niecelny strzał\", \n",
    "        \"interwencja VAR\", \"zmiana zawodnika\", \"przewinienie taktyczne\", \n",
    "        \"kontuzja\", \"decyzja o dogrywce\"\n",
    "    ],\n",
    "    \"AKCJA\": [\"wznowić grę\", \"rozpocząć grę\", \"przerwać grę\"]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd35923",
   "metadata": {},
   "source": [
    "## NLP and RDF Setup\n",
    "This section initializes the NLP tools (spaCy and Stanza) and sets up the RDF graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import stanza\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from rdflib import Graph, Namespace, URIRef, RDF, RDFS, Literal\n",
    "\n",
    "\n",
    "# Spacy initialization\n",
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "nlp.max_length = 2000000\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Stanza initialization\n",
    "stanza.download('pl')\n",
    "nlp_stanza = stanza.Pipeline('pl')\n",
    "\n",
    "# RDF setup\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "g = Graph()\n",
    "g.bind(\"ex\", EX)\n",
    "g.bind(\"rdfs\", RDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f430324",
   "metadata": {},
   "source": [
    "## Helper Functions for Entity Matching\n",
    "This section defines helper functions to find the longest matching entity and to map tokens to entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_for_token(token, doc, resources):\n",
    "    \"\"\"\n",
    "    Returns the longest entity from resources that covers the token in the document.\n",
    "    \"\"\"\n",
    "    for ent_text in sorted(resources, key=len, reverse=True):\n",
    "        for i in range(len(doc) - len(ent_text.split()) + 1):\n",
    "            span = doc[i:i+len(ent_text.split())]\n",
    "            if all(t.lemma_.lower() == e for t, e in zip(span, ent_text.split())):\n",
    "                if token.i >= span.start and token.i < span.end:\n",
    "                    return ent_text\n",
    "    return token.lemma_.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8cd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_entity(text, resources):\n",
    "    \"\"\"\n",
    "    Returns the longest entity from resources that contains the text.\n",
    "    \"\"\"\n",
    "    matches = [ent_text for ent_text in resources if text in ent_text]\n",
    "    if matches:\n",
    "        # Return the longest matching phrase\n",
    "        return max(matches, key=len)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00763f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_covering_entity(token_idx, entity_spans):\n",
    "    \"\"\"\n",
    "    Finds the longest entity that covers the token at the given index.\n",
    "    \"\"\"\n",
    "    covering = [ent for (start, end, ent) in entity_spans if start <= token_idx < end]\n",
    "    if covering:\n",
    "        # choose the longest (most words)\n",
    "        return max(covering, key=lambda x: len(x.split()))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b7edee",
   "metadata": {},
   "source": [
    "## RDF Graph Construction\n",
    "This section processes the extracted text, matches entities and actions, and builds the RDF graph with relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import URIRef, RDF, RDFS, Literal\n",
    "\n",
    "\n",
    "\n",
    "# Add patterns to PhraseMatcher\n",
    "for label, terms in entities.items():\n",
    "    patterns = [nlp.make_doc(term) for term in terms]\n",
    "    phrase_matcher.add(label, patterns)\n",
    "\n",
    "# Add patterns to Matcher\n",
    "for label, terms in entities.items():\n",
    "    for term in terms:\n",
    "        matcher.add(label, [[{\"LEMMA\": t} for t in term.split()]])\n",
    "\n",
    "# Example sentence\n",
    "#text = \"VAR referee analyzes the situation on the field. The player takes a penalty kick. The player cannot touch the ball with his hand.\"\n",
    "#extracted_text = text\n",
    "# NLP Spacy\n",
    "doc = nlp(extracted_text)\n",
    "\n",
    "# PhraseMatcher matching\n",
    "phrase_matches = phrase_matcher(doc)\n",
    "\n",
    "# Matcher matching\n",
    "lemma_matches = matcher(doc)\n",
    "\n",
    "matches = phrase_matches + lemma_matches\n",
    "\n",
    "# Create RDF entities\n",
    "resources = {}\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    label = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    span_lemma = \" \".join([token.lemma_.lower() for token in span])\n",
    "    span_lemma = span_lemma.lower()\n",
    "    if span_lemma not in resources:\n",
    "        uri = URIRef(EX[clean_for_uri(span_lemma)])\n",
    "        resources[span_lemma] = uri\n",
    "\n",
    "\n",
    "# Add Stanza analysis (add negation handling at the Stanza heuristic level)\n",
    "doc_stanza = nlp_stanza(extracted_text)\n",
    "all_found_entities = []  # Collect entities from all sentences\n",
    "for sentence in doc_stanza.sentences:\n",
    "    lemmas = [w.lemma.lower() for w in sentence.words]\n",
    "    # Find all entities in the sentence (longest matching phrases)\n",
    "    found_entities = []\n",
    "    used = set()\n",
    "    for ent_text in sorted(resources, key=lambda x: -len(x.split())):\n",
    "        ent_lemmas = ent_text.split()\n",
    "        for i in range(len(lemmas) - len(ent_lemmas) + 1):\n",
    "            if set(range(i, i+len(ent_lemmas))) & used:\n",
    "                continue\n",
    "            if lemmas[i:i+len(ent_lemmas)] == ent_lemmas:\n",
    "                found_entities.append((i, i+len(ent_lemmas), ent_text))\n",
    "                used.update(range(i, i+len(ent_lemmas)))\n",
    "    found_entities = sorted(found_entities, key=lambda x: x[0])\n",
    "\n",
    "    # After building found_entities:\n",
    "    filtered_entities = []\n",
    "    for i, (start_i, end_i, ent_i) in enumerate(found_entities):\n",
    "        is_sub = False\n",
    "        for j, (start_j, end_j, ent_j) in enumerate(found_entities):\n",
    "            if i != j and start_j <= start_i and end_j >= end_i and (end_j - start_j) > (end_i - start_i):\n",
    "                is_sub = True\n",
    "                break\n",
    "        if not is_sub:\n",
    "            filtered_entities.append((start_i, end_i, ent_i))\n",
    "    found_entities = filtered_entities\n",
    "\n",
    "    all_found_entities.extend(found_entities)\n",
    "\n",
    "\n",
    "    # Find verbs\n",
    "    verbs = [w for w in sentence.words if w.upos == 'VERB']\n",
    "\n",
    "    # Simple heuristic: entity before the verb is subject, after the verb is object\n",
    "    for verb in verbs:\n",
    "        verb_lemma = verb.lemma.lower()\n",
    "        # Negation detection in Stanza: is there 'nie' or upos == 'PART' with text 'nie' among dependents\n",
    "        is_negated = any((w.deprel == 'advmod:neg' or w.text.lower() == 'nie') for w in sentence.words if w.head == verb.id)\n",
    "        verb_predicate = f\"nie_{verb_lemma}\" if is_negated else verb_lemma\n",
    "        verb_uri = URIRef(EX[clean_for_uri(verb_predicate)])\n",
    "        # Find the entity closest before the verb\n",
    "        subj = None\n",
    "        obj = None\n",
    "        for start, end, ent in found_entities:\n",
    "            if end <= verb.id - 1:\n",
    "                subj = ent\n",
    "            elif start >= verb.id:\n",
    "                obj = ent\n",
    "                break\n",
    "        if subj:\n",
    "            subj_uri = resources.get(subj)\n",
    "            if subj_uri:\n",
    "                g.add((subj_uri, EX['wykonuje'], verb_uri))\n",
    "        if obj:\n",
    "            obj_uri = resources.get(obj)\n",
    "            if obj_uri:\n",
    "                g.add((verb_uri, EX['dotyczy'], obj_uri))\n",
    "        if subj and obj:\n",
    "            print(f\"Dodano RDF (heurystyka): {subj} --wykonuje--> {verb_predicate} --dotyczy--> {obj}\")\n",
    "\n",
    "# --- GLOBAL DEDUPLICATION AND ADD ONLY THE LONGEST, NON-OVERLAPPING ENTITIES TO RDF ---\n",
    "# Collect all entities (ent_text) from all sentences\n",
    "all_entity_spans = [(start, end, ent_text) for (start, end, ent_text) in all_found_entities]\n",
    "\n",
    "# Sort: first by start, then by length (descending), then by text length (descending)\n",
    "all_entity_spans = sorted(all_entity_spans, key=lambda x: (x[0], -(x[1]-x[0]), -len(x[2])))\n",
    "final_spans = []\n",
    "final_entities = set()\n",
    "for i, (start_i, end_i, ent_i) in enumerate(all_entity_spans):\n",
    "    overlap = False\n",
    "    for (start_j, end_j, ent_j) in final_spans:\n",
    "        # Check if ranges overlap\n",
    "        if not (end_i <= start_j or start_i >= end_j):\n",
    "            overlap = True\n",
    "            break\n",
    "    if not overlap:\n",
    "        final_spans.append((start_i, end_i, ent_i))\n",
    "        final_entities.add(ent_i)\n",
    "\n",
    "# Spacy dependency analysis (prefer the longest entities from final_spans) with negation handling\n",
    "for token in doc:\n",
    "    if token.pos_ == \"VERB\":\n",
    "        verb = token.lemma_.lower()\n",
    "        # Negation detection (e.g. 'nie' as a child of the verb or dep_ == 'neg')\n",
    "        is_negated = any(child.dep_ == \"neg\" or child.text.lower() == \"nie\" for child in token.children)\n",
    "        verb_predicate = f\"nie_{verb}\" if is_negated else verb\n",
    "        subject = [child for child in token.children if child.dep_ in (\"nsubj\", \"nsubj:pass\")]\n",
    "        obj = [child for child in token.children if child.dep_ in (\"obj\", \"dobj\", \"obl\")]\n",
    "\n",
    "        # Prefer the longest entity covering the subject/object token (from final_spans)\n",
    "        if subject and obj:\n",
    "            subj_ent = find_covering_entity(subject[0].i, final_spans) or subject[0].lemma_.lower()\n",
    "            obj_ent = find_covering_entity(obj[0].i, final_spans) or obj[0].lemma_.lower()\n",
    "            subj_uri = resources.get(subj_ent)\n",
    "            obj_uri = resources.get(obj_ent)\n",
    "            if subj_uri and obj_uri:\n",
    "                g.add((subj_uri, EX[clean_for_uri(verb_predicate)], obj_uri))\n",
    "        # Location and other adverbials\n",
    "        for child in token.children:\n",
    "            if child.dep_ == \"obl\":\n",
    "                loc_text = \" \".join([t.text.lower() for t in child.subtree])\n",
    "                loc_ent = find_longest_entity(loc_text, resources)\n",
    "                loc_uri = resources.get(loc_ent)\n",
    "                if loc_uri:\n",
    "                    g.add((URIRef(EX[clean_for_uri(verb_predicate)]), EX[\"miejsce\"], loc_uri))\n",
    "                    \n",
    "# Add RDF type/label only for final_entities (unique, non-overlapping entities)\n",
    "for ent_text in final_entities:\n",
    "    uri = resources[ent_text]\n",
    "    # Find label (type) from Spacy entities\n",
    "    label = None\n",
    "    for match_id, s, e in matches:\n",
    "        span = doc[s:e]\n",
    "        span_lemma = \" \".join([token.lemma_.lower() for token in span]).lower()\n",
    "        if span_lemma == ent_text:\n",
    "            label = nlp.vocab.strings[match_id]\n",
    "            break\n",
    "    if label:\n",
    "        class_uri = URIRef(EX[clean_for_uri(label.capitalize())])\n",
    "        if (uri, RDF.type, class_uri) not in g:\n",
    "            g.add((uri, RDF.type, class_uri))\n",
    "        # Add only ONE label: original text from the document (not lemma, no duplicates!)\n",
    "        # Collect all original texts for this entity (preserve case)\n",
    "        orig_texts = set()\n",
    "        for match_id, s, e in matches:\n",
    "            span = doc[s:e]\n",
    "            span_lemma = \" \".join([token.lemma_.lower() for token in span]).lower()\n",
    "            if span_lemma == ent_text:\n",
    "                orig_texts.add(span.text)\n",
    "        # Add only ONE label: original text from the document (longest, if there are different versions)\n",
    "        if orig_texts:\n",
    "            # Prefer the longest (e.g. \"VAR referee\" instead of \"referee\")\n",
    "            best_label = max(orig_texts, key=len)\n",
    "            if (uri, RDFS.label, Literal(best_label)) not in g:\n",
    "                g.add((uri, RDFS.label, Literal(best_label)))\n",
    "\n",
    "# Serialization\n",
    "#print(g.serialize(format=\"turtle\", encoding=\"utf-8\").decode(\"utf-8\"))\n",
    "output_file = \"graf5s.rdf\"\n",
    "g.serialize(destination=output_file, format=\"turtle\", encoding=\"utf-8\")\n",
    "print(f\"RDF graph has been saved to file: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv39)",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
